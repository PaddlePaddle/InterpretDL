# Reproducible Source Codes for the $M^4$ XAI Benchmark

The code can be found [here](https://github.com/holyseven/M4_XAI_Benchmark).

Note that this code is for the reproducible purpose. 
A wrapper for these modulars is on the plan.

This code is implemented using PaddlePaddle and InterpretDL.
A demo using HuggingFace models and InterpretDL can be found [here](https://github.com/holyseven/M4_XAI_Benchmark/blob/main/medical_image_example.ipynb).
A demo using Pytorch models and InterpretDL can be found [here](https://colab.research.google.com/drive/1ZgI1ctCc2ryPk0bdPgkEwQCJ1tHZCq14?usp=sharin).

Paper: 

> Xuhong Li, Mengnan Du, Jiamin Chen, Yekun Chai, Himabindu Lakkaraju, Haoyi Xiong. “M4: A Unified XAI Benchmark for Faithfulness Evaluation of Feature Attribution Methods across Metrics, Modalities and Models.” Neurips 2023, Dataset and Benchmark Track. https://openreview.net/forum?id=6zcfrSz98y.
